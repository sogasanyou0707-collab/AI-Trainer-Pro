import streamlit as st
import os
import json
import requests
import gspread
from google.oauth2.service_account import Credentials
import google.generativeai as genai
from datetime import datetime

# --- 1. è¨­å®šãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç† (ã‚¨ãƒ©ãƒ¼é˜²æ­¢ãƒ­ã‚¸ãƒƒã‚¯) ---
CONFIG_FILE = "app_settings.json"
SHEET_NAME = "Profiles"

def load_cache():
    """è¨­å®šã‚’èª­ã¿è¾¼ã¿ã€ä¸è¶³é …ç›®ãŒã‚ã‚Œã°è‡ªå‹•è£œå®Œã™ã‚‹"""
    defaults = {
        "user_name": "ç®¡ç†è€…",
        "user_role": "å°‚é–€ã‚¹ã‚¿ãƒƒãƒ•",
        "selected_model": "gemini-3-pro",
        "line_token": "",
        "line_user_id": ""
    }
    if os.path.exists(CONFIG_FILE):
        try:
            with open(CONFIG_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
                # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ãƒãƒ¼ã‚¸ã—ã¦å…¨é …ç›®ã‚’æƒãˆã‚‹
                defaults.update(data)
        except:
            pass
    return defaults

def save_cache(settings):
    with open(CONFIG_FILE, "w", encoding="utf-8") as f:
        json.dump(settings, f, indent=4, ensure_ascii=False)

# --- 2. å¤–éƒ¨é€£æºãƒ­ã‚¸ãƒƒã‚¯ ---
def get_latest_models():
    """1.5ç³»ã‚’é™¤å¤–ã—ãŸæœ€æ–°ãƒ¢ãƒ‡ãƒ«ã‚’å‹•çš„ã«å–å¾—"""
    try:
        genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
        models = [m.name.replace('models/', '') for m in genai.list_models() 
                if 'generateContent' in m.supported_generation_methods and "1.5" not in m.name]
        return models if models else ["gemini-3-pro"]
    except:
        return ["gemini-3-pro"]

def sync_line_info():
    """Secretsã‚’ä½¿ç”¨ã—ã¦ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰LINEæƒ…å ±ã‚’åŒæœŸ"""
    try:
        creds_info = st.secrets["connections"]["gsheets"]
        scopes = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']
        creds = Credentials.from_service_account_info(creds_info, scopes=scopes)
        client = gspread.authorize(creds)
        sh = client.open_by_url(creds_info["spreadsheet"])
        sheet = sh.worksheet(SHEET_NAME)
        # E2: Token, F2: User ID
        return sheet.acell('E2').value, sheet.acell('F2').value
    except Exception as e:
        st.error(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆåŒæœŸå¤±æ•—: {e}")
        return None, None

def ai_get_suggestions(content, model_name, role):
    """å…¥åŠ›å†…å®¹ã«åŸºã¥ãAIãŒã‚¿ã‚¹ã‚¯ã‚’ææ¡ˆ"""
    try:
        genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
        model = genai.GenerativeModel(model_name)
        prompt = f"ã‚ãªãŸã¯{role}ã®å°‚é–€ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®æ¥­å‹™å ±å‘Šã«åŸºã¥ãã€æ˜æ—¥ä»¥é™ã®ã‚¿ã‚¹ã‚¯ã‚’3ã¤å…·ä½“çš„ã«ææ¡ˆã—ã¦ãã ã•ã„ã€‚\n\nå†…å®¹:\n{content}"
        return model.generate_content(prompt).text
    except Exception as e:
        return f"ææ¡ˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"

# --- 3. UI æ§‹ç¯‰ (ã‚·ãƒ³ã‚°ãƒ«ã‚«ãƒ©ãƒ ãƒ»ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ) ---
st.set_page_config(page_title="AI Trainer Pro", layout="centered")

# åˆå›èµ·å‹•æ™‚ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«æ ¼ç´
if 'cache' not in st.session_state:
    st.session_state.cache = load_cache()
cache = st.session_state.cache

st.title("AI Trainer æ¥­å‹™å ±å‘Š")

# A. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ»æ—¥ä»˜æƒ…å ± (ã‚·ãƒ³ãƒ—ãƒ«è¡¨ç¤º)
st.write(f"ğŸ‘¤ **{cache.get('user_name')}** ({cache.get('user_role')})")
selected_date = st.date_input("å ±å‘Šæ—¥ã‚’é¸æŠ", datetime.now())

st.write("---")

# B. æ¥­å‹™å ±å‘Šå…¥åŠ›
report_text = st.text_area("æœ¬æ—¥ã®å ±å‘Šå†…å®¹", placeholder="ã“ã¡ã‚‰ã«æ¥­å‹™å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", height=250)

# C. ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒœã‚¿ãƒ³ (ç¸¦ã«é…ç½®)
if st.button("ğŸš€ LINEã§å ±å‘Šã‚’é€ä¿¡", use_container_width=True):
    if cache.get("line_token") and cache.get("line_user_id"):
        msg = f"ã€{selected_date} å ±å‘Šã€‘\næ‹…å½“: {cache['user_name']}\n---\n{report_text}"
        headers = {"Content-Type": "application/json", "Authorization": f"Bearer {cache['line_token']}"}
        data = {"to": cache["line_user_id"], "messages": [{"type": "text", "text": msg}]}
        
        with st.spinner("é€ä¿¡ä¸­..."):
            res = requests.post("https://api.line.me/v2/bot/message/push", headers=headers, json=data)
            if res.status_code == 200:
                st.success("LINEã«é€ä¿¡ã—ã¾ã—ãŸï¼")
            else:
                st.error("é€ä¿¡å¤±æ•—ã€‚è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    else:
        st.warning("è¨­å®šç”»é¢ã‹ã‚‰LINEæƒ…å ±ã‚’åŒæœŸã—ã¦ãã ã•ã„ã€‚")

if st.button("ğŸ’¡ AIã«ã‚¿ã‚¹ã‚¯ã‚’ç›¸è«‡ã™ã‚‹", use_container_width=True):
    if report_text:
        with st.spinner("æ€è€ƒä¸­..."):
            suggestions = ai_get_suggestions(report_text, cache['selected_model'], cache['user_role'])
            st.markdown("### AIã‹ã‚‰ã®ææ¡ˆ")
            st.info(suggestions)
    else:
        st.warning("å…ˆã«å ±å‘Šå†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

# --- 4. ã‚µã‚¤ãƒ‰ãƒãƒ¼ (è©³ç´°è¨­å®š) ---
with st.sidebar:
    st.header("âš™ï¸ ã‚·ã‚¹ãƒ†ãƒ è¨­å®š")
    with st.expander("è©³ç´°è¨­å®šã‚’é–‹ã", expanded=False):
        st.subheader("ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«")
        cache["user_name"] = st.text_input("è¡¨ç¤ºå", cache.get("user_name"))
        cache["user_role"] = st.text_input("å½¹å‰²", cache.get("user_role"))
        
        st.write("---")
        st.subheader("AIãƒ»é€£æºè¨­å®š")
        # ãƒ¢ãƒ‡ãƒ«é¸æŠ
        models = get_latest_models()
        cur_model = cache.get("selected_model", "gemini-3-pro")
        idx = models.index(cur_model) if cur_model in models else 0
        cache["selected_model"] = st.selectbox("ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«", models, index=idx)
        
        # LINEæƒ…å ±åŒæœŸãƒœã‚¿ãƒ³
        if st.button("LINEæƒ…å ±ã®åŒæœŸ", use_container_width=True):
            t, u = sync_line_info()
            if t and u:
                cache["line_token"], cache["line_user_id"] = t, u
                st.success("LINEæƒ…å ±ã‚’åŒæœŸã—ã¾ã—ãŸ")
        
        if st.button("è¨­å®šã‚’ä¿å­˜", use_container_width=True):
            save_cache(cache)
            st.toast("è¨­å®šã‚’ä¿å­˜ã—ã¾ã—ãŸ")

st.write("---")
st.caption(f"Last Sync: {datetime.now().strftime('%H:%M:%S')} / Model: {cache['selected_model']}")
